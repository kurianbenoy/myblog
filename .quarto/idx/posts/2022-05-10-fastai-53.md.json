{"title":"Deep Learning For Coders with fastai Couse - Lesson 3","markdown":{"yaml":{"title":"Deep Learning For Coders with fastai Couse - Lesson 3","type":"post","published":false,"tag":["fastbook","myself","ML","Deep learning","fastai","journey"],"readtime":true,"author":"kurianbenoy"},"containsRefs":false,"markdown":"\n\nThere is a minor delay in streaming the lessons today. Lesson 1, 2 are easy for everyone, while\nusually lesson 3 is more hard defenitely.\n\nFastai lesson 0, https://kurianbenoy.com/2021-06-16-fastgroup-1/\n\nIdea of running notebooks, understanding everything in that particular lesson.\nThen try to reproduce.(clean notebook approach, to test your understand)\nIf you can do with different dataset, ie the hard time.\n\nAlways study done, with other people is the best activity. This week, with the best no of votes.\n\nMy work also got featured, with lot of votes.\n\nToday Jeremy featured, a gradient descend platform. He has been using it for \npaperspace, and it's totally amazing. He got something done by them. He is going\nto add walk throughs of various lessons.\n\nIn lesson2, it's not taking a particular lesson and use in particular platform. There are two\nimportant pieces:\n\n1) Training piece\n2) Deployment piece\n\ntrain.ipynb\napp.ipynb - inference\n\nFinding good image models, by using and looking at a better architecture.\n\nHe tried levit_models, didn't work really great\n\ntimm.[25.00] - experiment with latest models. It got really good accuracy with\n0.0005. Now there are lot of good architectures, which beats resnet really well.\n22,000 categories of images. If you can do better, go do it.\n\n37 breeds, then in case of category. Usually that category is in the vocab of dataloaders.\n\nJust run learn.model. Understand what is stuff in learn?\n\nIn deep learning lot of trees\n\n[30:00] get modules in pytorch. Layer norm things\n\nWhat are these numbers? What are these parameters. How can these number figure out it's a   particular dog or not\n\nPartial application, it's just something in lot of languages. How applying quardartic\n\ndata matching function with data, adding noise with data\n`torch.linspace` which goes from -2 to +4\n\nCreate a plot quadratic, which helps in interacting. Using @interact\nmost simple and common loss functions MSE\n\nRerun with MSE. Now add MSE, and see if it get's better or worse. This is a manual process,\nand does he tweak. When we move up, does the loss get's better. Or if it decreases, does loss goes down.\n\n\nThe derivate is a function, which tells if we increase does it increase or decrease. In pytorch,\nyouu can automtically done by pytorch.\n\nipythonwidgets using intereact\n\nreturn interact. Does mse made by pytorch interact\n\nRank 1 tensor, 1 D tensor\n\n[49:00] onwards follow the calculation again, and try to do it again.\nNow it got and calculated the loss. \n\n```\nWith torch.no_grad():\n\n```\n\ninner part of machine learning code. This is basic optimizer. This is gradient descent.\nWe calculate gradient, and then build models. Not following stuff in lesson [57:00] is not clear for me.\n\nWe learned deep learned. This is like how to draw an owl. This is how deep learning is, just\ngo through the course.\n\nhttps://pytorch.org/tutorials/\n\nI want to be around 0.001 second. Doing grid search takes a lot of time. Training your good\nmodels, at the first day is not a big requirement. It's very easy to get inputs & outputs,\nyet getting segmented output is way harder. The important number, learning_rate to calculate parameters.\n\nAfter break\n\nMathematical trip, we want to do a whole bunch of RELUs. We want lot's of variables.\nAdd all the relus together, then use with different bunch of behaviours. 1000s of RELUs\n\nSIngle operation except last layer, with matrix multiplication.\nLinear algebra, almost all time you need is matrix multiplcator. It's multiplying\nand adding neural networks together. GPUs are so good at this.\n\nhttp://matrixmultiplication.xyz/\n\nfast.ai using excel for matrix multiplication. From [1:28:32]\n\nTitanic data, about who survived and who didnt'\n\nvalidation and metrics optimization. He is going to look into that Kaggle dataset.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"2022-05-10-fastai-53.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","theme":"sketchy","title-block-banner":true,"title":"Deep Learning For Coders with fastai Couse - Lesson 3","type":"post","published":false,"tag":["fastbook","myself","ML","Deep learning","fastai","journey"],"readtime":true,"author":"kurianbenoy"},"extensions":{"book":{"multiFile":true}}}}}